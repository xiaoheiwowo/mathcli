"""MathCLI - Command-line interface for math homework grading."""

import argparse
import json
import logging
import os
import random
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

try:
    from .ocr_processor import OCRProcessor
    from .ai_processor import AIProcessor, MathProblem
    from .markdown_grader import MarkdownGrader
    from .question_matcher import QuestionMatcher
    from .test_id_manager import TestIDManager
except ImportError:
    # Handle direct execution
    from ocr_processor import OCRProcessor
    from ai_processor import AIProcessor, MathProblem
    from markdown_grader import MarkdownGrader
    from question_matcher import QuestionMatcher
    from test_id_manager import TestIDManager


class ImageDataFilter(logging.Filter):
    """Custom filter to redact large image data from logs."""
    
    def filter(self, record):
        """Filter out or redact log messages containing large base64 image data."""
        if hasattr(record, 'msg') and record.msg:
            msg = str(record.msg)
            # Check if message contains base64 image data
            if 'data:image/' in msg and 'base64,' in msg:
                # Find and redact base64 data
                pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                redacted_msg = re.sub(pattern, 'data:image/[REDACTED_BASE64_IMAGE_DATA]', msg)
                record.msg = redacted_msg
            # Also check for very long strings that might be base64 data
            elif len(msg) > 1000 and re.search(r'[A-Za-z0-9+/=]{100,}', msg):
                # Truncate very long messages that likely contain base64 data
                if len(msg) > 2000:
                    record.msg = msg[:500] + '...[TRUNCATED_LARGE_DATA]...' + msg[-100:]
        return True


class MathGrader:
    """Main class for math homework grading."""
    
    def __init__(self, output_dir: str = "output"):
        """Initialize the math grader.
        
        Args:
            output_dir: Directory to save output files
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.ocr_processor = OCRProcessor()
        self.ai_processor = AIProcessor()
        self.question_matcher = QuestionMatcher()
        self.test_id_manager = TestIDManager()
        
        # Setup logging
        self.setup_logging()
    
    def setup_logging(self):
        """Setup logging configuration."""
        log_file = self.output_dir / "log.txt"
        
        # Create custom filter for image data
        image_filter = ImageDataFilter()
        
        # Configure logging with no truncation
        logging.basicConfig(
            level=logging.DEBUG,  # Changed to DEBUG for complete logging
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        # Apply image data filter to all handlers
        for handler in logging.getLogger().handlers:
            handler.addFilter(image_filter)
        
        # Ensure no log message truncation
        logging.getLogger().handlers[0].setLevel(logging.DEBUG)
        logging.getLogger().handlers[1].setLevel(logging.INFO)  # Keep console less verbose
        
        # Reduce verbosity of HTTP and OpenAI logs to avoid large image data in logs
        logging.getLogger('openai._base_client').setLevel(logging.WARNING)
        logging.getLogger('httpcore').setLevel(logging.WARNING)
        logging.getLogger('httpx').setLevel(logging.WARNING)
        
        self.logger = logging.getLogger(__name__)
    
    def process_image(self, image_path: str) -> Dict[str, Any]:
        """Process a math homework image and generate grading results.
        
        Args:
            image_path: Path to the homework image
            
        Returns:
            Complete grading results
        """
        self.logger.info(f"å¼€å§‹å¤„ç†å›¾ç‰‡: {image_path}")
        
        # Step 1: OCR Processing
        self.logger.info("æ­¥éª¤1: OCRæ–‡å­—è¯†åˆ«")
        ocr_result = self.ocr_processor.extract_text(image_path)
        
        if 'error' in ocr_result:
            self.logger.error(f"OCRå¤„ç†å¤±è´¥: {ocr_result['error']}")
            return {"error": "OCR processing failed", "details": ocr_result['error']}
        
        self.logger.info(f"OCRè¯†åˆ«å®Œæˆï¼Œç½®ä¿¡åº¦: {ocr_result['confidence']:.2f}%")
        self.logger.info(f"è¯†åˆ«æ–‡æœ¬:\n{ocr_result['raw_text']}")
        
        # Step 2: Parse into structured problems
        self.logger.info("æ­¥éª¤2: è§£ææ•°å­¦é¢˜ç›®")
        problems = self.ai_processor.parse_ocr_to_problems(ocr_result['raw_text'])
        self.logger.info(f"è¯†åˆ«åˆ° {len(problems)} é“é¢˜ç›®")
        
        # Step 3: Validate each problem
        self.logger.info("æ­¥éª¤3: éªŒè¯è§£ç­”è¿‡ç¨‹")
        results = []
        
        for i, problem in enumerate(problems, 1):
            self.logger.info(f"éªŒè¯ç¬¬ {i} é¢˜: {problem.problem_id}")
            validation = self.ai_processor.validate_solution(problem)
            feedback = self.ai_processor.generate_feedback(problem, validation)
            
            result = {
                "problem": {
                    "id": problem.problem_id,
                    "text": problem.problem_text,
                    "type": problem.problem_type,
                    "student_solution": {
                        "raw": problem.student_solution.raw,
                        "steps": [
                            {
                                "from": step.from_expr,
                                "to": step.to_expr,
                                "is_correct": step.is_correct,
                                "llm_determine": step.llm_determine,
                                "rule_determine": step.rule_determine
                            } for step in problem.student_solution.steps
                        ],
                        "calculation": [
                            {
                                "from": calc.from_expr,
                                "to": calc.to_expr,
                                "point": calc.point,
                                "is_correct": calc.is_correct
                            } for calc in problem.student_solution.calculation
                        ]
                    }
                },
                "validation": {
                    "is_correct": validation.is_correct,
                    "confidence": validation.confidence,
                    "error_type": validation.error_type,
                    "error_description": validation.error_description,
                    "suggestions": validation.suggestions
                },
                "feedback": feedback
            }
            
            results.append(result)
            
            status = "âœ“ æ­£ç¡®" if validation.is_correct else "âœ— é”™è¯¯"
            self.logger.info(f"ç¬¬ {i} é¢˜ç»“æœ: {status} (ç½®ä¿¡åº¦: {validation.confidence:.2f})")
            if not validation.is_correct:
                self.logger.info(f"é”™è¯¯ç±»å‹: {validation.error_type}")
                self.logger.info(f"é”™è¯¯æè¿°: {validation.error_description}")
        
        # Step 4: Generate summary
        correct_count = sum(1 for r in results if r['validation']['is_correct'])
        total_count = len(results)
        accuracy = (correct_count / total_count * 100) if total_count > 0 else 0
        
        summary = {
            "total_problems": total_count,
            "correct_problems": correct_count,
            "accuracy_percentage": accuracy,
            "processing_timestamp": datetime.now().isoformat(),
            "image_path": image_path,
            "ocr_confidence": ocr_result['confidence']
        }
        
        self.logger.info(f"æ­¥éª¤4: ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
        self.logger.info(f"æ€»é¢˜æ•°: {total_count}, æ­£ç¡®: {correct_count}, å‡†ç¡®ç‡: {accuracy:.1f}%")
        
        # Generate practice recommendations
        practice_recommendations = self._generate_practice_recommendations(results, summary)
        
        return {
            "summary": summary,
            "ocr_result": ocr_result,
            "problems": results,
            "practice_recommendations": practice_recommendations
        }
    
    def save_results(self, results: Dict[str, Any], filename: str = "result.json"):
        """Save results to JSON file.
        
        Args:
            results: Grading results
            filename: Output filename
        """
        output_file = self.output_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # Also create a markdown summary
        self.create_markdown_summary(results)
    
    def create_markdown_summary(self, results: Dict[str, Any]):
        """Create a markdown summary of the results."""
        summary_file = self.output_dir / "summary.md"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("# æ•°å­¦ä½œä¸šæ‰¹æ”¹æŠ¥å‘Š\n\n")
            
            summary = results['summary']
            f.write(f"**å¤„ç†æ—¶é—´**: {summary['processing_timestamp']}\n")
            f.write(f"**å›¾ç‰‡è·¯å¾„**: {summary['image_path']}\n")
            f.write(f"**OCRç½®ä¿¡åº¦**: {summary['ocr_confidence']:.2f}%\n\n")
            
            f.write("## æ€»ä½“ç»“æœ\n\n")
            f.write(f"- æ€»é¢˜æ•°: {summary['total_problems']}\n")
            f.write(f"- æ­£ç¡®é¢˜æ•°: {summary['correct_problems']}\n")
            f.write(f"- å‡†ç¡®ç‡: {summary['accuracy_percentage']:.1f}%\n\n")
            
            f.write("## è¯¦ç»†åˆ†æ\n\n")
            
            for i, problem_result in enumerate(results['problems'], 1):
                problem = problem_result['problem']
                validation = problem_result['validation']
                
                status = "âœ… æ­£ç¡®" if validation['is_correct'] else "âŒ é”™è¯¯"
                f.write(f"### ç¬¬ {i} é¢˜ {status}\n\n")
                f.write(f"**é¢˜ç›®**: {problem['text']}\n")
                f.write(f"**ç±»å‹**: {problem['type']}\n")
                f.write(f"**ç½®ä¿¡åº¦**: {validation['confidence']:.2f}\n\n")
                
                if problem['student_solution']:
                    f.write("**å­¦ç”Ÿè§£ç­”**:\n```\n")
                    # Handle both dict and string formats
                    if isinstance(problem['student_solution'], dict):
                        f.write(problem['student_solution'].get('raw', str(problem['student_solution'])))
                    else:
                        f.write(str(problem['student_solution']))
                    f.write("\n```\n\n")
                
                if not validation['is_correct']:
                    f.write(f"**é”™è¯¯ç±»å‹**: {validation['error_type']}\n")
                    f.write(f"**é”™è¯¯æè¿°**: {validation['error_description']}\n")
                    
                    if validation['suggestions']:
                        f.write("**å»ºè®®**:\n")
                        for suggestion in validation['suggestions']:
                            f.write(f"- {suggestion}\n")
                    f.write("\n")
        
        self.logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
    
    def _generate_practice_recommendations(self, results: List[Dict], summary: Dict) -> Dict[str, Any]:
        """Generate practice recommendations based on student performance."""
        total_problems = summary['total_problems']
        correct_problems = summary['correct_problems']
        accuracy = summary['accuracy_percentage']
        
        # Analyze error patterns
        error_patterns = {
            'fraction_operations': 0,
            'decimal_operations': 0,
            'mixed_operations': 0,
            'calculation_errors': 0,
            'concept_errors': 0,
            'step_logic_errors': 0
        }
        
        common_mistakes = []
        weak_areas = []
        
        for result in results:
            if not result['validation']['is_correct']:
                problem = result['problem']
                validation = result['validation']
                
                # Analyze problem type and errors
                problem_text = problem.get('text', '')
                
                # Check for fraction operations
                if '/' in problem_text:
                    error_patterns['fraction_operations'] += 1
                    
                # Check for decimal operations  
                if '.' in problem_text:
                    error_patterns['decimal_operations'] += 1
                    
                # Check for mixed operations
                if any(op in problem_text for op in ['Ã—', 'Ã·', '+', '-']) and len([op for op in ['Ã—', 'Ã·', '+', '-'] if op in problem_text]) > 1:
                    error_patterns['mixed_operations'] += 1
                
                # Analyze error description for specific issues
                error_desc = validation.get('error_description', '')
                if 'æ•°å­¦è®¡ç®—é”™è¯¯' in error_desc:
                    error_patterns['calculation_errors'] += 1
                if 'æ­¥éª¤' in error_desc:
                    error_patterns['step_logic_errors'] += 1
        
        # Generate recommendations based on performance
        recommendations = {
            'overall_assessment': self._get_overall_assessment(accuracy),
            'priority_areas': self._identify_priority_areas(error_patterns, total_problems),
            'specific_suggestions': self._get_specific_suggestions(error_patterns, accuracy),
            'practice_focus': self._get_practice_focus(error_patterns, accuracy),
            'next_steps': self._get_next_steps(accuracy, total_problems)
        }
        
        return recommendations
    
    def _get_overall_assessment(self, accuracy: float) -> str:
        """Get overall performance assessment."""
        if accuracy >= 90:
            return "ä¼˜ç§€ï¼æ•°å­¦åŸºç¡€æ‰å®ï¼Œè®¡ç®—å‡†ç¡®æ€§å¾ˆé«˜ã€‚"
        elif accuracy >= 80:
            return "è‰¯å¥½ï¼å¤§éƒ¨åˆ†é¢˜ç›®éƒ½èƒ½æ­£ç¡®è§£ç­”ï¼Œè¿˜æœ‰å°å¹…æå‡ç©ºé—´ã€‚"
        elif accuracy >= 60:
            return "åŠæ ¼ï¼åŸºæœ¬æ¦‚å¿µæŒæ¡ï¼Œä½†åœ¨è®¡ç®—å‡†ç¡®æ€§ä¸Šéœ€è¦åŠ å¼ºç»ƒä¹ ã€‚"
        else:
            return "éœ€è¦åŠ å¼ºï¼å»ºè®®é‡ç‚¹å¤ä¹ åŸºç¡€æ¦‚å¿µå’Œè®¡ç®—æ–¹æ³•ã€‚"
    
    def _identify_priority_areas(self, error_patterns: Dict, total_problems: int) -> List[str]:
        """Identify priority areas for improvement."""
        priority_areas = []
        
        if error_patterns['fraction_operations'] > 0:
            priority_areas.append("åˆ†æ•°è¿ç®—")
        if error_patterns['decimal_operations'] > 0:
            priority_areas.append("å°æ•°è¿ç®—")
        if error_patterns['mixed_operations'] > 0:
            priority_areas.append("æ··åˆè¿ç®—")
        if error_patterns['calculation_errors'] > total_problems * 0.3:
            priority_areas.append("è®¡ç®—å‡†ç¡®æ€§")
        if error_patterns['step_logic_errors'] > 0:
            priority_areas.append("è§£é¢˜æ­¥éª¤é€»è¾‘")
            
        return priority_areas if priority_areas else ["ç»§ç»­ä¿æŒå½“å‰æ°´å¹³"]
    
    def _get_specific_suggestions(self, error_patterns: Dict, accuracy: float) -> List[str]:
        """Get specific practice suggestions."""
        suggestions = []
        
        if error_patterns['fraction_operations'] > 0:
            suggestions.append("åŠ å¼ºåˆ†æ•°çš„åŠ å‡ä¹˜é™¤è¿ç®—ç»ƒä¹ ï¼Œç‰¹åˆ«æ³¨æ„é€šåˆ†å’Œçº¦åˆ†")
        if error_patterns['decimal_operations'] > 0:
            suggestions.append("ç»ƒä¹ å°æ•°çš„å››åˆ™è¿ç®—ï¼Œæ³¨æ„å°æ•°ç‚¹ä½ç½®çš„å¯¹é½")
        if error_patterns['mixed_operations'] > 0:
            suggestions.append("å¤ä¹ è¿ç®—é¡ºåºè§„åˆ™ï¼Œå…ˆä¹˜é™¤ååŠ å‡ï¼Œæœ‰æ‹¬å·å…ˆç®—æ‹¬å·å†…")
        if error_patterns['calculation_errors'] > 0:
            suggestions.append("æé«˜è®¡ç®—å‡†ç¡®æ€§ï¼Œå»ºè®®å¤šåšåŸºç¡€è®¡ç®—ç»ƒä¹ ")
        if error_patterns['step_logic_errors'] > 0:
            suggestions.append("æ³¨æ„è§£é¢˜æ­¥éª¤çš„é€»è¾‘æ€§ï¼Œæ¯ä¸€æ­¥éƒ½è¦æœ‰æ˜ç¡®çš„æ•°å­¦ä¾æ®")
            
        if accuracy >= 90:
            suggestions.append("å¯ä»¥å°è¯•æ›´å¤æ‚çš„ç»¼åˆæ€§é¢˜ç›®æ¥æŒ‘æˆ˜è‡ªå·±")
        elif accuracy < 60:
            suggestions.append("å»ºè®®ä»åŸºç¡€é¢˜ç›®å¼€å§‹ï¼Œé€æ­¥æé«˜éš¾åº¦")
            
        return suggestions if suggestions else ["ç»§ç»­ä¿æŒè‰¯å¥½çš„å­¦ä¹ ä¹ æƒ¯"]
    
    def _get_practice_focus(self, error_patterns: Dict, accuracy: float) -> Dict[str, str]:
        """Get focused practice recommendations."""
        focus = {}
        
        if error_patterns['fraction_operations'] > 0:
            focus['åˆ†æ•°è¿ç®—'] = "æ¯å¤©ç»ƒä¹ 10é“åˆ†æ•°åŠ å‡æ³•å’Œ10é“åˆ†æ•°ä¹˜é™¤æ³•"
        if error_patterns['mixed_operations'] > 0:
            focus['æ··åˆè¿ç®—'] = "æ¯å¤©ç»ƒä¹ 5é“å«æœ‰æ‹¬å·çš„æ··åˆè¿ç®—é¢˜"
        if accuracy < 80:
            focus['åŸºç¡€è®¡ç®—'] = "æ¯å¤©ç»ƒä¹ 20é“åŸºç¡€å››åˆ™è¿ç®—é¢˜ï¼Œæé«˜è®¡ç®—é€Ÿåº¦å’Œå‡†ç¡®æ€§"
        if accuracy >= 90:
            focus['æé«˜ç»ƒä¹ '] = "å¯ä»¥ç»ƒä¹ æ›´å¤æ‚çš„åº”ç”¨é¢˜å’Œç»¼åˆæ€§è®¡ç®—é¢˜"
            
        return focus
    
    def _get_next_steps(self, accuracy: float, total_problems: int) -> List[str]:
        """Get next steps recommendations."""
        steps = []
        
        if accuracy < 60:
            steps.extend([
                "1. å¤ä¹ åŸºç¡€æ¦‚å¿µå’Œè¿ç®—æ³•åˆ™",
                "2. æ¯å¤©å®ŒæˆåŸºç¡€è®¡ç®—ç»ƒä¹ ",
                "3. é‡ç‚¹å…³æ³¨é”™è¯¯é¢˜ç›®çš„è§£é¢˜æ–¹æ³•"
            ])
        elif accuracy < 80:
            steps.extend([
                "1. é’ˆå¯¹é”™è¯¯é¢˜ç›®è¿›è¡Œä¸“é¡¹ç»ƒä¹ ",
                "2. æé«˜è®¡ç®—å‡†ç¡®æ€§å’Œé€Ÿåº¦",
                "3. åŠ å¼ºæ··åˆè¿ç®—çš„ç»ƒä¹ "
            ])
        else:
            steps.extend([
                "1. ä¿æŒå½“å‰è‰¯å¥½çš„å­¦ä¹ çŠ¶æ€",
                "2. å¯ä»¥å°è¯•æ›´æœ‰æŒ‘æˆ˜æ€§çš„é¢˜ç›®",
                "3. å¸®åŠ©åŒå­¦è§£ç­”ç–‘é—®ï¼Œå·©å›ºçŸ¥è¯†"
            ])
            
        return steps
    
    def generate_practice_test(self, error_types: List[str], choice_count: int = 2, 
                              calculation_count: int = 2, random_seed: Optional[int] = None) -> Dict[str, Any]:
        """æ ¹æ®é”™è¯¯ç±»å‹ç”Ÿæˆç»ƒä¹ è¯•å·
        
        Args:
            error_types: é”™è¯¯ç±»å‹åˆ—è¡¨
            choice_count: é€‰æ‹©é¢˜æ•°é‡
            calculation_count: è®¡ç®—é¢˜æ•°é‡
            random_seed: éšæœºç§å­ï¼Œç”¨äºæ§åˆ¶éšæœºæ€§ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç»ƒä¹ è¯•å·æ•°æ®
        """
        self.logger.info(f"å¼€å§‹ç”Ÿæˆç»ƒä¹ è¯•å·ï¼Œé”™è¯¯ç±»å‹: {error_types}")
        
        try:
            # è®¾ç½®éšæœºç§å­
            import random
            if random_seed is not None:
                random.seed(random_seed)
                self.logger.info(f"ä½¿ç”¨éšæœºç§å­: {random_seed}")
            else:
                # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡ç”Ÿæˆéƒ½ä¸åŒ
                random.seed()
                self.logger.info("ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºéšæœºç§å­")
            
            # åŠ è½½é¢˜åº“æ•°æ®
            question_db = self._load_question_database()
            if not question_db:
                return {"error": "æ— æ³•åŠ è½½é¢˜åº“æ•°æ®"}
            
            # æ ¹æ®é”™è¯¯ç±»å‹ç­›é€‰é¢˜ç›®
            difficulty_range = None  # å¯ä»¥ä»å‚æ•°ä¸­è·å–
            selected_questions = self._select_questions_by_error_types(
                question_db, error_types, choice_count, calculation_count, difficulty_range
            )
            
            if not selected_questions:
                return {"error": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é¢˜ç›®"}
            
            # ç»Ÿè®¡é¢˜ç›®ç±»å‹
            choice_count_actual = 0
            calculation_count_actual = 0
            
            for question in selected_questions:
                question_type = question.get('question_type', '')
                has_choices = 'choices' in question and question['choices']
                
                if question_type == 'choice' or has_choices:
                    choice_count_actual += 1
                elif question_type == 'calculation' or (not has_choices and question_type != 'choice'):
                    calculation_count_actual += 1
            
            # ç”Ÿæˆç»ƒä¹ è¯•å·ï¼ˆç§»é™¤ç­”æ¡ˆå’Œè§£é¢˜è¿‡ç¨‹ï¼‰
            practice_questions = []
            question_ids = []
            
            for question in selected_questions:
                question_id = question.get('id', '')
                question_ids.append(question_id)
                
                # åˆ›å»ºä¸åŒ…å«ç­”æ¡ˆçš„é¢˜ç›®å‰¯æœ¬
                practice_question = {
                    "id": question_id,
                    "question_info": question.get('question_info', {}),
                    "question_type": question.get('question_type', ''),
                }
                
                # åªä¿ç•™é€‰æ‹©é¢˜çš„é€‰é¡¹å†…å®¹ï¼Œä¸åŒ…å«æ­£ç¡®ç­”æ¡ˆæ ‡è¯†
                if 'choices' in question and question['choices']:
                    practice_choices = []
                    for choice in question['choices']:
                        practice_choices.append({
                            "id": choice.get('id', ''),
                            "content": choice.get('content', '')
                        })
                    practice_question['choices'] = practice_choices
                
                practice_questions.append(practice_question)
            
            # ç”Ÿæˆè¯•å·ID
            test_id = self.test_id_manager.generate_test_id(question_ids, "practice")
            
            # ä¸ºæ¯é“é¢˜ç›®æ·»åŠ é¢˜ç›®ç¼–å·
            for i, question in enumerate(practice_questions, 1):
                question["question_number"] = f"ç¬¬ {i} é¢˜"
            
            # ç”Ÿæˆç»ƒä¹ è¯•å·
            practice_test = {
                "test_info": {
                    "test_id": test_id,
                    "generated_at": datetime.now().isoformat(),
                    "error_types": error_types,
                    "total_questions": len(practice_questions),
                    "choice_questions": choice_count_actual,
                    "calculation_questions": calculation_count_actual,
                    "test_type": "practice"
                },
                "questions": practice_questions,
                "instructions": self._generate_test_instructions(error_types)
            }
            
            # ä¿å­˜è¯•å·è®°å½•åˆ°æ•°æ®åº“
            test_info = {
                "test_type": "practice",
                "error_types": error_types,
                "choice_count": choice_count_actual,
                "calculation_count": calculation_count_actual,
                "generated_at": practice_test["test_info"]["generated_at"]
            }
            
            save_success = self.test_id_manager.save_test_record(test_id, question_ids, test_info)
            if save_success:
                self.logger.info(f"è¯•å·è®°å½•å·²ä¿å­˜åˆ°æ•°æ®åº“: {test_id}")
            else:
                self.logger.warning(f"è¯•å·è®°å½•ä¿å­˜å¤±è´¥: {test_id}")
            
            self.logger.info(f"ç»ƒä¹ è¯•å·ç”Ÿæˆå®Œæˆï¼Œå…± {len(selected_questions)} é“é¢˜ç›®")
            return practice_test
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç»ƒä¹ è¯•å·æ—¶å‡ºé”™: {e}")
            return {"error": f"ç”Ÿæˆç»ƒä¹ è¯•å·å¤±è´¥: {str(e)}"}
    
    def _load_question_database(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½é¢˜åº“æ•°æ®ï¼Œä»databaseç›®å½•è¯»å–"""
        try:
            database_dir = Path(__file__).parent / "database"
            all_questions = []
            
            # å®šä¹‰è¦åŠ è½½çš„é¢˜åº“æ–‡ä»¶åˆ—è¡¨
            question_bank_files = [
                "db_question_bank.json",
                "db_question_bank_choice.json"
            ]
            
            for json_file in question_bank_files:
                file_path = database_dir / json_file
                if file_path.exists():
                    self.logger.info(f"åŠ è½½é¢˜åº“æ–‡ä»¶: {json_file}")
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                        # æå–é¢˜ç›®æ•°æ®
                        questions = data.get('question_bank', {}).get('questions', [])
                        all_questions.extend(questions)
                        self.logger.info(f"ä» {json_file} åŠ è½½äº† {len(questions)} é“é¢˜ç›®")
            
            if not all_questions:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢˜ç›®æ•°æ®")
                return None
            
            # åˆ›å»ºç»Ÿä¸€çš„é¢˜åº“ç»“æ„
            question_database = {
                "version": "1.0",
                "metadata": {
                    "description": "åˆå¹¶é¢˜åº“æ•°æ®",
                    "total_questions": len(all_questions),
                    "source_files": question_bank_files
                },
                "question_bank": {
                    "questions": all_questions
                }
            }
            
            self.logger.info(f"æ€»å…±åŠ è½½äº† {len(all_questions)} é“é¢˜ç›®")
            return question_database
            
        except Exception as e:
            self.logger.error(f"åŠ è½½é¢˜åº“æ•°æ®å¤±è´¥: {e}")
            return None
    
    
    def _select_questions_by_error_types(self, question_db: Dict[str, Any], 
                                       error_types: List[str], choice_count: int, 
                                       calculation_count: int, difficulty_range: Optional[Tuple[str, str]] = None) -> List[Dict[str, Any]]:
        """æ ¹æ®é”™è¯¯ç±»å‹é€‰æ‹©é¢˜ç›®"""
        selected_questions = []
        
        # è·å–æ‰€æœ‰é¢˜ç›®
        all_questions = question_db.get('question_bank', {}).get('questions', [])
        
        if not all_questions:
            self.logger.warning("é¢˜åº“ä¸­æ²¡æœ‰é¢˜ç›®")
            return selected_questions
        
        self.logger.info(f"é¢˜åº“ä¸­å…±æœ‰ {len(all_questions)} é“é¢˜ç›®")
        
        # åˆ†ç¦»é€‰æ‹©é¢˜å’Œè®¡ç®—é¢˜
        choice_questions = []
        calculation_questions = []
        
        for question in all_questions:
            if self._question_matches_error_types(question, error_types):
                # åˆ¤æ–­é¢˜ç›®ç±»å‹
                question_type = question.get('question_type', '')
                has_choices = 'choices' in question and question['choices']
                
                if question_type == 'choice' or has_choices:
                    choice_questions.append(question)
                elif question_type == 'calculation' or (not has_choices and question_type != 'choice'):
                    calculation_questions.append(question)
        
        self.logger.info(f"åŒ¹é…çš„é¢˜ç›®: é€‰æ‹©é¢˜ {len(choice_questions)} é“, è®¡ç®—é¢˜ {len(calculation_questions)} é“")
        
        # å¢å¼ºçš„éšæœºé€‰æ‹©ç®—æ³•
        
        # éšæœºæ‰“ä¹±é¢˜ç›®é¡ºåºï¼Œå¢åŠ éšæœºæ€§
        random.shuffle(choice_questions)
        random.shuffle(calculation_questions)
        
        # å¦‚æœæŒ‡å®šäº†éš¾åº¦èŒƒå›´ï¼Œè¿›ä¸€æ­¥ç­›é€‰
        if difficulty_range:
            choice_questions = self._filter_by_difficulty(choice_questions, difficulty_range)
            calculation_questions = self._filter_by_difficulty(calculation_questions, difficulty_range)
        
        # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„é¢˜ç›®
        if choice_questions:
            selected_choice_count = min(choice_count, len(choice_questions))
            # ä½¿ç”¨åŠ æƒéšæœºé€‰æ‹©ï¼Œä¼˜å…ˆé€‰æ‹©éš¾åº¦é€‚ä¸­çš„é¢˜ç›®
            selected_choice = self._weighted_random_selection(choice_questions, selected_choice_count)
            selected_questions.extend(selected_choice)
            self.logger.info(f"é€‰æ‹©äº† {len(selected_choice)} é“é€‰æ‹©é¢˜")
        
        if calculation_questions:
            selected_calc_count = min(calculation_count, len(calculation_questions))
            # ä½¿ç”¨åŠ æƒéšæœºé€‰æ‹©ï¼Œä¼˜å…ˆé€‰æ‹©éš¾åº¦é€‚ä¸­çš„é¢˜ç›®
            selected_calc = self._weighted_random_selection(calculation_questions, selected_calc_count)
            selected_questions.extend(selected_calc)
            self.logger.info(f"é€‰æ‹©äº† {len(selected_calc)} é“è®¡ç®—é¢˜")
        
        # æœ€ç»ˆéšæœºæ‰“ä¹±é¢˜ç›®é¡ºåº
        random.shuffle(selected_questions)
        
        return selected_questions
    
    def _filter_by_difficulty(self, questions: List[Dict[str, Any]], difficulty_range: Tuple[str, str]) -> List[Dict[str, Any]]:
        """æ ¹æ®éš¾åº¦èŒƒå›´ç­›é€‰é¢˜ç›®"""
        min_difficulty, max_difficulty = difficulty_range
        difficulty_order = ['easy', 'medium', 'hard']
        
        try:
            min_idx = difficulty_order.index(min_difficulty)
            max_idx = difficulty_order.index(max_difficulty)
        except ValueError:
            return questions
        
        filtered_questions = []
        for question in questions:
            difficulty = question.get('question_info', {}).get('difficulty', 'medium')
            if difficulty in difficulty_order:
                diff_idx = difficulty_order.index(difficulty)
                if min_idx <= diff_idx <= max_idx:
                    filtered_questions.append(question)
        
        return filtered_questions
    
    def _weighted_random_selection(self, questions: List[Dict[str, Any]], count: int) -> List[Dict[str, Any]]:
        """åŠ æƒéšæœºé€‰æ‹©é¢˜ç›®ï¼Œä¼˜å…ˆé€‰æ‹©éš¾åº¦é€‚ä¸­çš„é¢˜ç›®"""
        if not questions or count <= 0:
            return []
        
        if count >= len(questions):
            return questions.copy()
        
        # è®¡ç®—æ¯ä¸ªé¢˜ç›®çš„æƒé‡
        weights = []
        for question in questions:
            difficulty = question.get('question_info', {}).get('difficulty', 'medium')
            
            # éš¾åº¦æƒé‡ï¼šeasy=1, medium=2, hard=1.5
            if difficulty == 'easy':
                weight = 1.0
            elif difficulty == 'medium':
                weight = 2.0  # ä¼˜å…ˆé€‰æ‹©ä¸­ç­‰éš¾åº¦
            elif difficulty == 'hard':
                weight = 1.5
            else:
                weight = 1.0
            
            # æ·»åŠ éšæœºå› å­ï¼Œé¿å…æ€»æ˜¯é€‰æ‹©ç›¸åŒçš„é¢˜ç›®
            weight += random.uniform(0, 0.5)
            weights.append(weight)
        
        # ä½¿ç”¨åŠ æƒéšæœºé€‰æ‹©
        selected_indices = random.choices(
            range(len(questions)), 
            weights=weights, 
            k=count
        )
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        selected_questions = []
        seen_indices = set()
        for idx in selected_indices:
            if idx not in seen_indices:
                selected_questions.append(questions[idx])
                seen_indices.add(idx)
        
        # å¦‚æœå»é‡åæ•°é‡ä¸è¶³ï¼Œè¡¥å……éšæœºé€‰æ‹©
        while len(selected_questions) < count:
            remaining_indices = [i for i in range(len(questions)) if i not in seen_indices]
            if not remaining_indices:
                break
            idx = random.choice(remaining_indices)
            selected_questions.append(questions[idx])
            seen_indices.add(idx)
        
        return selected_questions
    
    def _question_matches_error_types(self, question: Dict[str, Any], error_types: List[str]) -> bool:
        """æ£€æŸ¥é¢˜ç›®æ˜¯å¦åŒ¹é…æŒ‡å®šçš„é”™è¯¯ç±»å‹"""
        # è·å–é¢˜ç›®æ ‡ç­¾
        tags = question.get('question_info', {}).get('tags', [])
        if not tags:
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå°è¯•ä»é¢˜ç›®æ–‡æœ¬ä¸­æ¨æ–­
            text = question.get('question_info', {}).get('text', '') or question.get('question', {}).get('text', '')
            tags = self._extract_tags_from_text(text)
        
        # ä¸­æ–‡é”™è¯¯ç±»å‹æ˜ å°„åˆ°æ ‡ç­¾
        error_type_mapping = {
            'ç¬¦å·é”™è¯¯': ['è´Ÿæ•°è¿ç®—', 'ç¬¦å·', 'è´Ÿå·', 'æ­£è´Ÿå·', 'æœ‰ç†æ•°', 'ä¹˜æ–¹'],
            'è®¡ç®—é”™è¯¯': ['è®¡ç®—', 'è¿ç®—', 'å››åˆ™è¿ç®—', 'ç®—æœ¯'],
            'åˆ†æ•°è¿ç®—': ['åˆ†æ•°', 'åˆ†æ•°è¿ç®—'],
            'å°æ•°è¿ç®—': ['å°æ•°', 'å°æ•°è¿ç®—'],
            'æ··åˆè¿ç®—': ['æ··åˆè¿ç®—', 'ç»¼åˆè¿ç®—'],
            'æ–¹ç¨‹': ['æ–¹ç¨‹', 'ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹', 'è§£æ–¹ç¨‹'],
            'ä¹˜æ–¹': ['ä¹˜æ–¹', 'å¹‚è¿ç®—']
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•é”™è¯¯ç±»å‹åŒ¹é…é¢˜ç›®æ ‡ç­¾
        for error_type in error_types:
            if error_type in error_type_mapping:
                if any(tag in tags for tag in error_type_mapping[error_type]):
                    return True
        
        return False
    
    def _extract_tags_from_text(self, text: str) -> List[str]:
        """ä»é¢˜ç›®æ–‡æœ¬ä¸­æå–æ ‡ç­¾"""
        tags = []
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['åˆ†æ•°', 'fraction', '/']):
            tags.append('åˆ†æ•°è¿ç®—')
        if any(word in text_lower for word in ['å°æ•°', 'decimal', '.']):
            tags.append('å°æ•°è¿ç®—')
        if any(word in text_lower for word in ['è´Ÿæ•°', 'negative', '-']):
            tags.append('è´Ÿæ•°è¿ç®—')
        if any(word in text_lower for word in ['æ–¹ç¨‹', 'equation', '=']):
            tags.append('æ–¹ç¨‹')
        if any(word in text_lower for word in ['ä¹˜æ–¹', 'power', '^']):
            tags.append('ä¹˜æ–¹')
        
        return tags
    
    def _generate_test_instructions(self, error_types: List[str]) -> str:
        """ç”Ÿæˆæµ‹è¯•è¯´æ˜"""
        # ç›´æ¥ä½¿ç”¨ä¸­æ–‡é”™è¯¯ç±»å‹ä½œä¸ºç»ƒä¹ é‡ç‚¹
        focus_areas = error_types
        
        instructions = f"""
ç»ƒä¹ è¯•å·è¯´æ˜ï¼š
æœ¬è¯•å·é‡ç‚¹ç»ƒä¹ ï¼š{', '.join(focus_areas)}

ç­”é¢˜è¦æ±‚ï¼š
1. é€‰æ‹©é¢˜è¯·é€‰æ‹©æ­£ç¡®ç­”æ¡ˆ
2. è®¡ç®—é¢˜è¯·å†™å‡ºå®Œæ•´çš„è§£é¢˜è¿‡ç¨‹
3. æ³¨æ„è¿ç®—ç¬¦å·çš„æ­£ç¡®ä½¿ç”¨
4. ä»”ç»†æ£€æŸ¥è®¡ç®—ç»“æœ

ç¥å­¦ä¹ è¿›æ­¥ï¼
        """.strip()
        
        return instructions
    
    def save_practice_test(self, practice_test: Dict[str, Any], filename: str = "practice_test.json"):
        """ä¿å­˜ç»ƒä¹ è¯•å·åˆ°æ–‡ä»¶
        
        Args:
            practice_test: ç»ƒä¹ è¯•å·æ•°æ®
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        output_file = self.output_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(practice_test, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»ƒä¹ è¯•å·å·²ä¿å­˜åˆ°: {output_file}")
        
        # åŒæ—¶åˆ›å»ºMarkdownæ ¼å¼çš„è¯•å·
        self.create_practice_test_markdown(practice_test)
    
    def create_practice_test_markdown(self, practice_test: Dict[str, Any]):
        """åˆ›å»ºMarkdownæ ¼å¼çš„ç»ƒä¹ è¯•å·"""
        md_file = self.output_dir / "practice_test.md"
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# æ•°å­¦ç»ƒä¹ è¯•å·\n\n")
            
            # å­¦ç”Ÿä¿¡æ¯å¡«å†™åŒºåŸŸ
            f.write("## å­¦ç”Ÿä¿¡æ¯\n\n")
            f.write("**å§“å**: _________________\n\n")
            f.write("**å­¦å·**: _________________\n\n")
            f.write("**ç­çº§**: _________________\n\n")
            f.write("---\n\n")
            
            # è¯•å·ä¿¡æ¯
            f.write("## è¯•å·ä¿¡æ¯\n\n")
            test_info = practice_test['test_info']

            f.write(f"**ç”Ÿæˆæ—¶é—´**: {test_info['generated_at']}\n")
            f.write(f"**é‡ç‚¹ç»ƒä¹ **: {', '.join(test_info['error_types'])}\n")
            f.write(f"**é¢˜ç›®æ€»æ•°**: {test_info['total_questions']}\n")
            f.write(f"**é€‰æ‹©é¢˜**: {test_info['choice_questions']} é“\n")
            f.write(f"**è®¡ç®—é¢˜**: {test_info['calculation_questions']} é“\n")
            f.write(f"**è¯•å·ID**: {test_info.get('test_id', 'N/A')}\n\n")
            
            # è¯´æ˜
            f.write("## ç­”é¢˜è¯´æ˜\n\n")
            f.write(practice_test['instructions'])
            f.write("\n\n")
            
            # é¢˜ç›®
            f.write("## é¢˜ç›®\n\n")
            for i, question in enumerate(practice_test['questions'], 1):
                f.write(f"### ç¬¬ {i} é¢˜\n\n")
                
                # é¢˜ç›®æ–‡æœ¬
                question_text = question.get('question_info', {}).get('text', '') or question.get('question', {}).get('text', '')
                f.write(f"**é¢˜ç›®**: {question_text}\n\n")
                
                # é€‰æ‹©é¢˜é€‰é¡¹ï¼ˆä¸æ˜¾ç¤ºæ­£ç¡®ç­”æ¡ˆæ ‡è¯†ï¼‰
                if 'choices' in question and question['choices']:
                    f.write("**é€‰é¡¹**:\n")
                    for choice in question['choices']:
                        choice_id = choice.get('id', '')
                        choice_content = choice.get('content', '')
                        f.write(f"- {choice_id}. {choice_content}\n")
                    f.write("\n")
                    f.write("**ç­”æ¡ˆ**: (    )\n\n")
                
                # ä¸ºè®¡ç®—é¢˜ç•™å‡ºç­”é¢˜ç©ºé—´
                question_type = question.get('question_type', '')
                has_choices = 'choices' in question and question['choices']
                
                if question_type == 'calculation' or (not has_choices and question_type != 'choice'):
                    f.write("**è§£ç­”**: \n\n")
                    f.write("```\n")
                    f.write("è¯·åœ¨æ­¤å¤„å†™å‡ºå®Œæ•´çš„è§£é¢˜è¿‡ç¨‹\n")
                    f.write("```\n\n")
                
                f.write("---\n\n")
            
            # åœ¨è¯•å·æœ«å°¾æ·»åŠ è¯•å·ID
            f.write("---\n\n")
            f.write("## è¯•å·æ ‡è¯†\n\n")
            f.write(f"**è¯•å·ID**: {test_info.get('test_id', 'N/A')}\n\n")
            f.write("*è¯·ä¿ç•™æ­¤è¯•å·IDï¼Œç”¨äºåç»­æ‰¹æ”¹å’Œæˆç»©æŸ¥è¯¢*\n")
        
        self.logger.info(f"Markdownè¯•å·å·²ä¿å­˜åˆ°: {md_file}")
    
    def save_student_answer(self, answer_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å­¦ç”Ÿç­”é¢˜è®°å½•åˆ°æ•°æ®åº“
        
        Args:
            answer_data: ç­”é¢˜æ•°æ®
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            database_dir = Path(__file__).parent / "database"
            answer_file = database_dir / "db_answer.json"
            
            # åŠ è½½ç°æœ‰ç­”é¢˜è®°å½•
            if answer_file.exists():
                with open(answer_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = {
                    "version": "1.0",
                    "metadata": {"description": "å­¦ç”Ÿç­”é¢˜è®°å½•æ•°æ®åº“", "total_answers": 0},
                    "answer_records": [],
                    "sessions": [],
                    "statistics": {"total_sessions": 0, "total_answers": 0, "overall_accuracy": 0.0}
                }
            
            # æ·»åŠ æ–°çš„ç­”é¢˜è®°å½•
            data["answer_records"].append(answer_data)
            data["metadata"]["total_answers"] = len(data["answer_records"])
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(answer_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ç­”é¢˜è®°å½•å·²ä¿å­˜: {answer_data.get('answer_id', 'unknown')}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç­”é¢˜è®°å½•å¤±è´¥: {e}")
            return False
    
    def get_student_errors(self, student_id: str) -> List[Dict[str, Any]]:
        """è·å–å­¦ç”Ÿçš„é”™è¯¯ç­”é¢˜è®°å½•
        
        Args:
            student_id: å­¦ç”ŸID
            
        Returns:
            é”™è¯¯ç­”é¢˜è®°å½•åˆ—è¡¨
        """
        try:
            database_dir = Path(__file__).parent / "database"
            answer_file = database_dir / "db_answer.json"
            
            if not answer_file.exists():
                return []
            
            with open(answer_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ç­›é€‰è¯¥å­¦ç”Ÿçš„é”™è¯¯ç­”é¢˜
            error_answers = []
            for record in data.get("answer_records", []):
                if (record.get("student_id") == student_id and 
                    not record.get("result", {}).get("is_correct", True)):
                    error_answers.append(record)
            
            return error_answers
            
        except Exception as e:
            self.logger.error(f"è·å–å­¦ç”Ÿé”™è¯¯è®°å½•å¤±è´¥: {e}")
            return []
    
    def analyze_common_errors(self) -> List[Dict[str, Any]]:
        """åˆ†æå¸¸è§é”™è¯¯ç±»å‹
        
        Returns:
            å¸¸è§é”™è¯¯åˆ†æç»“æœ
        """
        try:
            database_dir = Path(__file__).parent / "database"
            answer_file = database_dir / "db_answer.json"
            
            if not answer_file.exists():
                return []
            
            with open(answer_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ç»Ÿè®¡é”™è¯¯ç±»å‹
            error_counts = {}
            for record in data.get("answer_records", []):
                if not record.get("result", {}).get("is_correct", True):
                    error_analysis = record.get("error_analysis", {})
                    if error_analysis:
                        error_type = error_analysis.get("primary_error", "æœªçŸ¥é”™è¯¯")
                        error_counts[error_type] = error_counts.get(error_type, 0) + 1
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            common_errors = []
            total_errors = sum(error_counts.values())
            for error_type, count in error_counts.items():
                common_errors.append({
                    "error_type": error_type,
                    "count": count,
                    "percentage": (count / total_errors * 100) if total_errors > 0 else 0
                })
            
            # æŒ‰æ•°é‡æ’åº
            common_errors.sort(key=lambda x: x["count"], reverse=True)
            return common_errors
            
        except Exception as e:
            self.logger.error(f"åˆ†æå¸¸è§é”™è¯¯å¤±è´¥: {e}")
            return []
    
    def generate_question_bank_statistics(self) -> Dict[str, Any]:
        """ç”Ÿæˆé¢˜åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            # åŠ è½½é¢˜åº“æ•°æ®
            question_db = self._load_question_database()
            if not question_db:
                return {"error": "æ— æ³•åŠ è½½é¢˜åº“æ•°æ®"}
            
            all_questions = question_db.get('question_bank', {}).get('questions', [])
            
            # åŸºæœ¬ç»Ÿè®¡
            total_questions = len(all_questions)
            
            # é¢˜å‹ç»Ÿè®¡
            question_types = {}
            for question in all_questions:
                q_type = question.get('question_type', 'unknown')
                question_types[q_type] = question_types.get(q_type, 0) + 1
            
            # éš¾åº¦ç»Ÿè®¡
            difficulty_stats = {}
            for question in all_questions:
                difficulty = question.get('question_info', {}).get('difficulty', 'unknown')
                difficulty_stats[difficulty] = difficulty_stats.get(difficulty, 0) + 1
            
            # ç« èŠ‚ç»Ÿè®¡
            chapter_stats = {}
            for question in all_questions:
                chapter = question.get('question_info', {}).get('chapter', 'æœªçŸ¥ç« èŠ‚')
                chapter_stats[chapter] = chapter_stats.get(chapter, 0) + 1
            
            # æ ‡ç­¾ç»Ÿè®¡
            tag_stats = {}
            for question in all_questions:
                tags = question.get('question_info', {}).get('tags', [])
                for tag in tags:
                    tag_stats[tag] = tag_stats.get(tag, 0) + 1
            
            # é”™è¯¯ç±»å‹åŒ¹é…ç»Ÿè®¡
            error_type_stats = {}
            error_type_mapping = {
                'ç¬¦å·é”™è¯¯': ['è´Ÿæ•°è¿ç®—', 'ç¬¦å·', 'è´Ÿå·', 'æ­£è´Ÿå·', 'æœ‰ç†æ•°', 'ä¹˜æ–¹'],
                'è®¡ç®—é”™è¯¯': ['è®¡ç®—', 'è¿ç®—', 'å››åˆ™è¿ç®—', 'ç®—æœ¯'],
                'åˆ†æ•°è¿ç®—': ['åˆ†æ•°', 'åˆ†æ•°è¿ç®—'],
                'å°æ•°è¿ç®—': ['å°æ•°', 'å°æ•°è¿ç®—'],
                'æ··åˆè¿ç®—': ['æ··åˆè¿ç®—', 'ç»¼åˆè¿ç®—'],
                'æ–¹ç¨‹': ['æ–¹ç¨‹', 'ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹', 'è§£æ–¹ç¨‹'],
                'ä¹˜æ–¹': ['ä¹˜æ–¹', 'å¹‚è¿ç®—']
            }
            
            for error_type, matching_tags in error_type_mapping.items():
                count = 0
                for question in all_questions:
                    question_tags = question.get('question_info', {}).get('tags', [])
                    if any(tag in question_tags for tag in matching_tags):
                        count += 1
                error_type_stats[error_type] = count
            
            # é¢„ä¼°æ—¶é—´ç»Ÿè®¡
            total_estimated_time = sum(
                question.get('question_info', {}).get('estimated_time', 0) 
                for question in all_questions
            )
            avg_estimated_time = total_estimated_time / total_questions if total_questions > 0 else 0
            
            # æ„å»ºç»Ÿè®¡ç»“æœ
            statistics = {
                "basic_info": {
                    "total_questions": total_questions,
                    "source_files": question_db.get('metadata', {}).get('source_files', []),
                    "last_updated": question_db.get('metadata', {}).get('last_updated', ''),
                    "total_estimated_time": total_estimated_time,
                    "average_estimated_time": round(avg_estimated_time, 1)
                },
                "question_types": question_types,
                "difficulty_distribution": difficulty_stats,
                "chapter_distribution": chapter_stats,
                "tag_distribution": dict(sorted(tag_stats.items(), key=lambda x: x[1], reverse=True)),
                "error_type_coverage": error_type_stats,
                "generated_at": datetime.now().isoformat()
            }
            
            self.logger.info(f"é¢˜åº“ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå®Œæˆï¼Œå…± {total_questions} é“é¢˜ç›®")
            return statistics
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé¢˜åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": f"ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"}
    
    def format_statistics_table(self, stats: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯ä¸ºè¡¨æ ¼å½¢å¼
        
        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
        """
        if "error" in stats:
            return f"é”™è¯¯: {stats['error']}"
        
        output = []
        output.append("=" * 60)
        output.append("ğŸ“Š æ•°å­¦é¢˜åº“ç»Ÿè®¡ä¿¡æ¯")
        output.append("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        basic_info = stats.get('basic_info', {})
        output.append(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯")
        output.append(f"æ€»é¢˜ç›®æ•°: {basic_info.get('total_questions', 0)}")
        output.append(f"æ¥æºæ–‡ä»¶: {', '.join(basic_info.get('source_files', []))}")
        output.append(f"æ€»é¢„ä¼°æ—¶é—´: {basic_info.get('total_estimated_time', 0)} åˆ†é’Ÿ")
        output.append(f"å¹³å‡é¢„ä¼°æ—¶é—´: {basic_info.get('average_estimated_time', 0)} åˆ†é’Ÿ/é¢˜")
        
        # é¢˜å‹åˆ†å¸ƒ
        output.append(f"\nğŸ“ é¢˜å‹åˆ†å¸ƒ")
        question_types = stats.get('question_types', {})
        for q_type, count in question_types.items():
            percentage = (count / basic_info.get('total_questions', 1)) * 100
            output.append(f"  {q_type}: {count} é¢˜ ({percentage:.1f}%)")
        
        # éš¾åº¦åˆ†å¸ƒ
        output.append(f"\nğŸ¯ éš¾åº¦åˆ†å¸ƒ")
        difficulty_stats = stats.get('difficulty_distribution', {})
        for difficulty, count in difficulty_stats.items():
            percentage = (count / basic_info.get('total_questions', 1)) * 100
            output.append(f"  {difficulty}: {count} é¢˜ ({percentage:.1f}%)")
        
        # ç« èŠ‚åˆ†å¸ƒ
        output.append(f"\nğŸ“š ç« èŠ‚åˆ†å¸ƒ")
        chapter_stats = stats.get('chapter_distribution', {})
        for chapter, count in chapter_stats.items():
            percentage = (count / basic_info.get('total_questions', 1)) * 100
            output.append(f"  {chapter}: {count} é¢˜ ({percentage:.1f}%)")
        
        # æ ‡ç­¾åˆ†å¸ƒï¼ˆå‰10ä¸ªï¼‰
        output.append(f"\nğŸ·ï¸  æ ‡ç­¾åˆ†å¸ƒ (å‰10ä¸ª)")
        tag_stats = stats.get('tag_distribution', {})
        for i, (tag, count) in enumerate(list(tag_stats.items())[:10], 1):
            percentage = (count / basic_info.get('total_questions', 1)) * 100
            output.append(f"  {i:2d}. {tag}: {count} é¢˜ ({percentage:.1f}%)")
        
        # é”™è¯¯ç±»å‹è¦†ç›–
        output.append(f"\nâŒ é”™è¯¯ç±»å‹è¦†ç›–")
        error_type_stats = stats.get('error_type_coverage', {})
        for error_type, count in error_type_stats.items():
            percentage = (count / basic_info.get('total_questions', 1)) * 100
            output.append(f"  {error_type}: {count} é¢˜ ({percentage:.1f}%)")
        
        output.append(f"\nâ° ç”Ÿæˆæ—¶é—´: {stats.get('generated_at', '')}")
        output.append("=" * 60)
        
        return "\n".join(output)
    
    def convert_image_to_markdown(self, image_path: str, format_type: str = 'practice', 
                                 output_filename: str = 'ocr_result.md') -> Dict[str, Any]:
        """å°†å›¾ç‰‡è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„è¯•å·å†…å®¹
        
        Args:
            image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
            format_type: è¯•å·æ ¼å¼ç±»å‹ (practice/exam/homework)
            output_filename: è¾“å‡ºMarkdownæ–‡ä»¶å
            
        Returns:
            è½¬æ¢ç»“æœå­—å…¸
        """
        self.logger.info(f"å¼€å§‹å°†å›¾ç‰‡è½¬æ¢ä¸ºMarkdown: {image_path}")
        
        try:
            # Step 1: OCR Processing
            self.logger.info("æ­¥éª¤1: OCRæ–‡å­—è¯†åˆ«")
            ocr_result = self.ocr_processor.extract_text(image_path)
            
            if 'error' in ocr_result:
                self.logger.error(f"OCRå¤„ç†å¤±è´¥: {ocr_result['error']}")
                return {"error": "OCR processing failed", "details": ocr_result['error']}
            
            self.logger.info(f"OCRè¯†åˆ«å®Œæˆï¼Œç½®ä¿¡åº¦: {ocr_result['confidence']:.2f}%")
            self.logger.info(f"è¯†åˆ«æ–‡æœ¬:\n{ocr_result['raw_text']}")
            
            # Step 2: Find matching questions in question bank
            self.logger.info("æ­¥éª¤2: åœ¨é¢˜åº“ä¸­æŸ¥æ‰¾åŒ¹é…é¢˜ç›®")
            matched_questions = self._find_matching_questions(ocr_result['raw_text'])
            
            # Step 3: Convert to Markdown using LLM with matched questions
            self.logger.info("æ­¥éª¤3: è½¬æ¢ä¸ºMarkdownæ ¼å¼")
            markdown_content = self._convert_ocr_to_markdown_with_matches(
                ocr_result['raw_text'], format_type, matched_questions
            )
            
            if not markdown_content:
                return {"error": "Failed to convert OCR text to Markdown"}
            
            # Step 3: Save Markdown file
            self.logger.info("æ­¥éª¤3: ä¿å­˜Markdownæ–‡ä»¶")
            output_file = self.output_dir / output_filename
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            self.logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file}")
            
            return {
                "success": True,
                "output_file": str(output_file),
                "confidence": ocr_result['confidence'],
                "method": ocr_result.get('method', 'unknown'),
                "format_type": format_type,
                "content_length": len(markdown_content),
                "matched_questions": matched_questions
            }
            
        except Exception as e:
            self.logger.error(f"å›¾ç‰‡è½¬Markdownå¤±è´¥: {e}")
            return {"error": f"Image to Markdown conversion failed: {str(e)}"}
    
    def _convert_ocr_to_markdown(self, ocr_text: str, format_type: str) -> str:
        """ä½¿ç”¨LLMå°†OCRæ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼
        
        Args:
            ocr_text: OCRè¯†åˆ«çš„åŸå§‹æ–‡æœ¬
            format_type: è¯•å·æ ¼å¼ç±»å‹
            
        Returns:
            Markdownæ ¼å¼çš„è¯•å·å†…å®¹
        """
        if not self.ai_processor.use_llm or not self.ai_processor.client:
            self.logger.warning("LLMä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€Markdownè½¬æ¢")
            return self._basic_ocr_to_markdown(ocr_text, format_type)
        
        try:
            # æ ¹æ®æ ¼å¼ç±»å‹é€‰æ‹©ä¸åŒçš„æç¤ºè¯
            format_prompts = {
                'practice': self._get_practice_format_prompt(),
                'exam': self._get_exam_format_prompt(),
                'homework': self._get_homework_format_prompt()
            }
            
            prompt = format_prompts.get(format_type, format_prompts['practice'])
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"""è¯·å°†ä»¥ä¸‹OCRè¯†åˆ«çš„æ•°å­¦è¯•å·å†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–çš„Markdownæ ¼å¼ã€‚

OCRè¯†åˆ«å†…å®¹ï¼š
{ocr_text}

{prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¦æ±‚è¾“å‡ºMarkdownå†…å®¹ï¼Œç¡®ä¿ï¼š
1. ä¿æŒåŸæœ‰çš„é¢˜ç›®é¡ºåºå’Œç¼–å·
2. æ­£ç¡®è¯†åˆ«é€‰æ‹©é¢˜å’Œè®¡ç®—é¢˜
3. ä¿æŒæ•°å­¦è¡¨è¾¾å¼çš„å‡†ç¡®æ€§
4. ä½¿ç”¨æ ‡å‡†çš„Markdownè¯­æ³•
5. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œç›´æ¥è¾“å‡ºMarkdownå†…å®¹"""

            response = self.ai_processor.client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {
                        'role': 'system',
                        'content': 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°å­¦è€å¸ˆï¼Œæ“…é•¿å°†OCRè¯†åˆ«çš„è¯•å·å†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–çš„Markdownæ ¼å¼ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºæ ¼å¼åŒ–çš„Markdownå†…å®¹ã€‚'
                    },
                    {'role': 'user', 'content': full_prompt}
                ],
                temperature=0.1,
                max_tokens=3000
            )
            
            markdown_content = response.choices[0].message.content.strip()
            self.logger.info("LLM Markdownè½¬æ¢å®Œæˆ")
            return markdown_content
            
        except Exception as e:
            self.logger.error(f"LLM Markdownè½¬æ¢å¤±è´¥: {e}")
            self.logger.info("å›é€€åˆ°åŸºç¡€Markdownè½¬æ¢")
            return self._basic_ocr_to_markdown(ocr_text, format_type)
    
    def _get_practice_format_prompt(self) -> str:
        """è·å–ç»ƒä¹ è¯•å·æ ¼å¼æç¤ºè¯"""
        return """è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š

# æ•°å­¦ç»ƒä¹ è¯•å·

## å­¦ç”Ÿä¿¡æ¯

**å§“å**: _________________

**å­¦å·**: _________________

**ç­çº§**: _________________

---

## é¢˜ç›®

### ç¬¬ X é¢˜

**é¢˜ç›®**: [é¢˜ç›®å†…å®¹]

**é€‰é¡¹**:
- a. [é€‰é¡¹A]
- b. [é€‰é¡¹B]
- c. [é€‰é¡¹C]
- d. [é€‰é¡¹D]

**ç­”æ¡ˆ**: (    )

---

### ç¬¬ X é¢˜

**é¢˜ç›®**: [é¢˜ç›®å†…å®¹]

**è§£ç­”**: 
[è§£é¢˜è¿‡ç¨‹]

```"""
    
    def _get_exam_format_prompt(self) -> str:
        """è·å–è€ƒè¯•è¯•å·æ ¼å¼æç¤ºè¯"""
        return """è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š

# æ•°å­¦è€ƒè¯•è¯•å·

## è€ƒè¯•ä¿¡æ¯

**è€ƒè¯•ç§‘ç›®**: æ•°å­¦

**è€ƒè¯•æ—¶é—´**: _________________

**ç­çº§**: _________________

**å§“å**: _________________

---

## é¢˜ç›®

### ä¸€ã€é€‰æ‹©é¢˜ï¼ˆæ¯é¢˜Xåˆ†ï¼Œå…±XXåˆ†ï¼‰

#### ç¬¬ X é¢˜

**é¢˜ç›®**: [é¢˜ç›®å†…å®¹]

**é€‰é¡¹**:
- A. [é€‰é¡¹A]
- B. [é€‰é¡¹B]
- C. [é€‰é¡¹C]
- D. [é€‰é¡¹D]

**ç­”æ¡ˆ**: (    )

### äºŒã€è®¡ç®—é¢˜ï¼ˆæ¯é¢˜Xåˆ†ï¼Œå…±XXåˆ†ï¼‰

#### ç¬¬ X é¢˜

**é¢˜ç›®**: [é¢˜ç›®å†…å®¹]

**è§£ç­”**: 
[è§£é¢˜è¿‡ç¨‹]"""
    
    def _get_homework_format_prompt(self) -> str:
        """è·å–ä½œä¸šæ ¼å¼æç¤ºè¯"""
        return """è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š

# æ•°å­¦ä½œä¸š

## å­¦ç”Ÿä¿¡æ¯

**å§“å**: _________________

**ç­çº§**: _________________

**æ—¥æœŸ**: _________________

---

## ä½œä¸šå†…å®¹

### ç¬¬ X é¢˜

**é¢˜ç›®**: [é¢˜ç›®å†…å®¹]

**è§£ç­”**: 
[è§£é¢˜è¿‡ç¨‹]

---"""
    
    def _basic_ocr_to_markdown(self, ocr_text: str, format_type: str) -> str:
        """åŸºç¡€OCRåˆ°Markdownè½¬æ¢ï¼ˆä¸ä½¿ç”¨LLMï¼‰"""
        import re
        
        # ç®€å•çš„æ–‡æœ¬å¤„ç†
        lines = ocr_text.split('\n')
        markdown_lines = []
        
        # æ·»åŠ æ ‡é¢˜
        if format_type == 'practice':
            markdown_lines.append("# æ•°å­¦ç»ƒä¹ è¯•å·")
        elif format_type == 'exam':
            markdown_lines.append("# æ•°å­¦è€ƒè¯•è¯•å·")
        else:
            markdown_lines.append("# æ•°å­¦ä½œä¸š")
        
        markdown_lines.append("")
        
        # æ·»åŠ å­¦ç”Ÿä¿¡æ¯åŒºåŸŸ
        markdown_lines.append("## å­¦ç”Ÿä¿¡æ¯")
        markdown_lines.append("")
        markdown_lines.append("**å§“å**: _________________")
        markdown_lines.append("")
        markdown_lines.append("**å­¦å·**: _________________")
        markdown_lines.append("")
        markdown_lines.append("**ç­çº§**: _________________")
        markdown_lines.append("")
        markdown_lines.append("---")
        markdown_lines.append("")
        
        # æ·»åŠ é¢˜ç›®åŒºåŸŸ
        markdown_lines.append("## é¢˜ç›®")
        markdown_lines.append("")
        
        # ç®€å•çš„é¢˜ç›®è¯†åˆ«å’Œæ ¼å¼åŒ–
        current_question = 1
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æµ‹é¢˜ç›®ç¼–å·
            if re.match(r'^\d+[\.\)]\s*', line):
                markdown_lines.append(f"### ç¬¬ {current_question} é¢˜")
                markdown_lines.append("")
                markdown_lines.append(f"**é¢˜ç›®**: {line}")
                markdown_lines.append("")
                markdown_lines.append("**è§£ç­”**: ")
                markdown_lines.append("")
                markdown_lines.append("```")
                markdown_lines.append("è¯·åœ¨æ­¤å¤„å†™å‡ºå®Œæ•´çš„è§£é¢˜è¿‡ç¨‹")
                markdown_lines.append("```")
                markdown_lines.append("")
                markdown_lines.append("---")
                markdown_lines.append("")
                current_question += 1
            else:
                # å…¶ä»–å†…å®¹ä½œä¸ºé¢˜ç›®æè¿°
                if not any(markdown_lines[-1].startswith(prefix) for prefix in ['**é¢˜ç›®**:', '**è§£ç­”**:', '```', '---']):
                    if markdown_lines and not markdown_lines[-1].startswith('**é¢˜ç›®**:'):
                        markdown_lines.append(f"**é¢˜ç›®**: {line}")
                    else:
                        markdown_lines.append(line)
        
        return '\n'.join(markdown_lines)
    
    def _find_matching_questions(self, ocr_text: str) -> List[Dict[str, Any]]:
        """åœ¨é¢˜åº“ä¸­æŸ¥æ‰¾åŒ¹é…çš„é¢˜ç›®
        
        Args:
            ocr_text: OCRè¯†åˆ«çš„æ–‡æœ¬
            
        Returns:
            åŒ¹é…çš„é¢˜ç›®åˆ—è¡¨
        """
        matched_questions = []
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²é¢˜ç›®
            import re
            # åŒ¹é…é¢˜ç›®ç¼–å·æ¨¡å¼
            question_patterns = [
                r'(\d+[\.\)]\s*[^0-9]+?)(?=\d+[\.\)]|$)',  # æ•°å­—ç¼–å·
                r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\)]\s*[^ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\)]|$)',  # ä¸­æ–‡ç¼–å·
            ]
            
            questions = []
            for pattern in question_patterns:
                matches = re.findall(pattern, ocr_text, re.DOTALL)
                questions.extend(matches)
            
            if not questions:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢˜ç›®åˆ†å‰²ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªé¢˜ç›®
                questions = [ocr_text]
            
            self.logger.info(f"ä»OCRæ–‡æœ¬ä¸­è¯†åˆ«åˆ° {len(questions)} ä¸ªé¢˜ç›®ç‰‡æ®µ")
            
            # ä¸ºæ¯ä¸ªé¢˜ç›®ç‰‡æ®µæŸ¥æ‰¾åŒ¹é…
            for i, question_text in enumerate(questions):
                question_text = question_text.strip()
                if not question_text:
                    continue
                
                self.logger.info(f"åŒ¹é…ç¬¬ {i+1} é¢˜: {question_text[:50]}...")
                
                # åˆ¤æ–­é¢˜ç›®ç±»å‹
                question_type = self._detect_question_type(question_text)
                
                # æŸ¥æ‰¾åŒ¹é…çš„é¢˜ç›®
                match_result = self.question_matcher.find_matching_question(question_text, question_type)
                
                if match_result:
                    matched_questions.append({
                        "ocr_text": question_text,
                        "matched_question": match_result["question"],
                        "similarity": match_result["similarity"],
                        "method": match_result["method"],
                        "question_index": i + 1
                    })
                    self.logger.info(f"ç¬¬ {i+1} é¢˜åŒ¹é…æˆåŠŸ: {match_result['question']['id']} (ç›¸ä¼¼åº¦: {match_result['similarity']:.3f})")
                else:
                    self.logger.warning(f"ç¬¬ {i+1} é¢˜æœªæ‰¾åˆ°åŒ¹é…: {question_text[:50]}...")
                    matched_questions.append({
                        "ocr_text": question_text,
                        "matched_question": None,
                        "similarity": 0.0,
                        "method": "no_match",
                        "question_index": i + 1
                    })
            
        except Exception as e:
            self.logger.error(f"é¢˜ç›®åŒ¹é…å¤±è´¥: {e}")
        
        return matched_questions
    
    def _detect_question_type(self, question_text: str) -> str:
        """æ£€æµ‹é¢˜ç›®ç±»å‹
        
        Args:
            question_text: é¢˜ç›®æ–‡æœ¬
            
        Returns:
            é¢˜ç›®ç±»å‹ (choice/calculation)
        """
        # æ£€æµ‹é€‰æ‹©é¢˜ç‰¹å¾
        choice_patterns = [
            r'[A-D][\.\)]',  # A. B. C. D.
            r'[a-d][\.\)]',  # a. b. c. d.
            r'é€‰æ‹©',  # åŒ…å«"é€‰æ‹©"å­—æ ·
            r'ä¸‹åˆ—.*?æ­£ç¡®',  # ä¸‹åˆ—...æ­£ç¡®
            r'å“ªä¸ª.*?æ˜¯',  # å“ªä¸ª...æ˜¯
        ]
        
        for pattern in choice_patterns:
            if re.search(pattern, question_text):
                return "choice"
        
        # æ£€æµ‹è®¡ç®—é¢˜ç‰¹å¾
        calc_patterns = [
            r'è®¡ç®—',  # åŒ…å«"è®¡ç®—"å­—æ ·
            r'æ±‚',  # åŒ…å«"æ±‚"å­—æ ·
            r'è§£',  # åŒ…å«"è§£"å­—æ ·
            r'[+\-*/=]',  # åŒ…å«æ•°å­¦è¿ç®—ç¬¦
        ]
        
        for pattern in calc_patterns:
            if re.search(pattern, question_text):
                return "calculation"
        
        return "calculation"  # é»˜è®¤ä¸ºè®¡ç®—é¢˜
    
    def _convert_ocr_to_markdown_with_matches(self, ocr_text: str, format_type: str, 
                                            matched_questions: List[Dict[str, Any]]) -> str:
        """ä½¿ç”¨åŒ¹é…çš„é¢˜ç›®ä¿¡æ¯è½¬æ¢ä¸ºMarkdownæ ¼å¼
        
        Args:
            ocr_text: OCRè¯†åˆ«çš„åŸå§‹æ–‡æœ¬
            format_type: è¯•å·æ ¼å¼ç±»å‹
            matched_questions: åŒ¹é…çš„é¢˜ç›®åˆ—è¡¨
            
        Returns:
            Markdownæ ¼å¼çš„è¯•å·å†…å®¹
        """
        if not self.ai_processor.use_llm or not self.ai_processor.client:
            self.logger.warning("LLMä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€Markdownè½¬æ¢")
            return self._basic_ocr_to_markdown(ocr_text, format_type)
        
        try:
            # æ„å»ºåŒ¹é…é¢˜ç›®ä¿¡æ¯
            match_info = self._build_match_info(matched_questions)
            
            # æ ¹æ®æ ¼å¼ç±»å‹é€‰æ‹©ä¸åŒçš„æç¤ºè¯
            format_prompts = {
                'practice': self._get_practice_format_prompt(),
                'exam': self._get_exam_format_prompt(),
                'homework': self._get_homework_format_prompt()
            }
            
            prompt = format_prompts.get(format_type, format_prompts['practice'])
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"""è¯·å°†ä»¥ä¸‹OCRè¯†åˆ«çš„æ•°å­¦è¯•å·å†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–çš„Markdownæ ¼å¼ã€‚

OCRè¯†åˆ«å†…å®¹ï¼š
{ocr_text}

{prompt}

åŒ¹é…çš„é¢˜åº“é¢˜ç›®ä¿¡æ¯ï¼š
{match_info}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¦æ±‚è¾“å‡ºMarkdownå†…å®¹ï¼Œå¹¶åˆ©ç”¨åŒ¹é…çš„é¢˜åº“é¢˜ç›®ä¿¡æ¯æ¥ï¼š
1. ä¿æŒåŸæœ‰çš„é¢˜ç›®é¡ºåºå’Œç¼–å·
2. æ­£ç¡®è¯†åˆ«é€‰æ‹©é¢˜å’Œè®¡ç®—é¢˜
3. ä½¿ç”¨é¢˜åº“ä¸­çš„æ ‡å‡†é¢˜ç›®å†…å®¹ï¼ˆå¦‚æœåŒ¹é…åº¦è¾ƒé«˜ï¼‰
4. ä¿æŒæ•°å­¦è¡¨è¾¾å¼çš„å‡†ç¡®æ€§
5. ä½¿ç”¨æ ‡å‡†çš„Markdownè¯­æ³•
6. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œç›´æ¥è¾“å‡ºMarkdownå†…å®¹"""

            response = self.ai_processor.client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {
                        'role': 'system',
                        'content': 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°å­¦è€å¸ˆï¼Œæ“…é•¿å°†OCRè¯†åˆ«çš„è¯•å·å†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–çš„Markdownæ ¼å¼ã€‚è¯·åˆ©ç”¨é¢˜åº“ä¿¡æ¯æé«˜è½¬æ¢è´¨é‡ï¼Œä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºæ ¼å¼åŒ–çš„Markdownå†…å®¹ã€‚'
                    },
                    {'role': 'user', 'content': full_prompt}
                ],
                temperature=0.1,
                max_tokens=4000
            )
            
            markdown_content = response.choices[0].message.content.strip()
            self.logger.info("LLM Markdownè½¬æ¢å®Œæˆï¼ˆä½¿ç”¨é¢˜åº“åŒ¹é…ï¼‰")
            return markdown_content
            
        except Exception as e:
            self.logger.error(f"LLM Markdownè½¬æ¢å¤±è´¥: {e}")
            self.logger.info("å›é€€åˆ°åŸºç¡€Markdownè½¬æ¢")
            return self._basic_ocr_to_markdown(ocr_text, format_type)
    
    def _build_match_info(self, matched_questions: List[Dict[str, Any]]) -> str:
        """æ„å»ºåŒ¹é…é¢˜ç›®ä¿¡æ¯å­—ç¬¦ä¸²
        
        Args:
            matched_questions: åŒ¹é…çš„é¢˜ç›®åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„åŒ¹é…ä¿¡æ¯å­—ç¬¦ä¸²
        """
        if not matched_questions:
            return "æœªæ‰¾åˆ°åŒ¹é…çš„é¢˜åº“é¢˜ç›®"
        
        info_lines = []
        for match in matched_questions:
            if match["matched_question"]:
                question = match["matched_question"]
                info_lines.append(f"é¢˜ç›® {match['question_index']}:")
                info_lines.append(f"  OCRæ–‡æœ¬: {match['ocr_text'][:100]}...")
                info_lines.append(f"  åŒ¹é…é¢˜ç›®ID: {question['id']}")
                info_lines.append(f"  é¢˜åº“é¢˜ç›®: {question.get('question_info', {}).get('text', '')[:100]}...")
                info_lines.append(f"  ç›¸ä¼¼åº¦: {match['similarity']:.3f}")
                info_lines.append(f"  åŒ¹é…æ–¹æ³•: {match['method']}")
                info_lines.append("")
            else:
                info_lines.append(f"é¢˜ç›® {match['question_index']}: æœªæ‰¾åˆ°åŒ¹é…")
                info_lines.append(f"  OCRæ–‡æœ¬: {match['ocr_text'][:100]}...")
                info_lines.append("")
        
        return "\n".join(info_lines)


def main() -> None:
    """Main entry point for the CLI application."""
    parser = argparse.ArgumentParser(description="MathCLI - æ•°å­¦ä½œä¸šæ‰¹æ”¹å·¥å…·")
    
    # åˆ›å»ºå­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ‰¹æ”¹å‘½ä»¤
    grade_parser = subparsers.add_parser('grade', help='æ‰¹æ”¹æ•°å­¦ä½œä¸š')
    grade_parser.add_argument("-i", "--image", required=True, help="è¾“å…¥å›¾ç‰‡è·¯å¾„")
    grade_parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½• (é»˜è®¤: output)")
    grade_parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    # ç”Ÿæˆç»ƒä¹ è¯•å·å‘½ä»¤
    practice_parser = subparsers.add_parser('practice', help='ç”Ÿæˆç»ƒä¹ è¯•å·')
    practice_parser.add_argument("-e", "--error-types", nargs='+', required=True, 
                                help="é”™è¯¯ç±»å‹åˆ—è¡¨ï¼Œå¦‚ï¼šç¬¦å·é”™è¯¯ è®¡ç®—é”™è¯¯ åˆ†æ•°è¿ç®— å°æ•°è¿ç®— æ··åˆè¿ç®—")
    practice_parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½• (é»˜è®¤: output)")
    practice_parser.add_argument("--choice-count", type=int, default=2, help="é€‰æ‹©é¢˜æ•°é‡ (é»˜è®¤: 2)")
    practice_parser.add_argument("--calculation-count", type=int, default=2, help="è®¡ç®—é¢˜æ•°é‡ (é»˜è®¤: 2)")
    practice_parser.add_argument("--random-seed", type=int, help="éšæœºç§å­ï¼Œç”¨äºæ§åˆ¶é¢˜ç›®ç”Ÿæˆçš„éšæœºæ€§")
    practice_parser.add_argument("--difficulty-range", nargs=2, choices=['easy', 'medium', 'hard'], 
                                help="éš¾åº¦èŒƒå›´ï¼Œå¦‚ï¼šeasy medium")
    practice_parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    # æŸ¥çœ‹é¢˜åº“ç»Ÿè®¡å‘½ä»¤
    stats_parser = subparsers.add_parser('stats', help='æŸ¥çœ‹é¢˜åº“ç»Ÿè®¡ä¿¡æ¯')
    stats_parser.add_argument("--format", choices=['table', 'json'], default='table', 
                             help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: table)")
    stats_parser.add_argument("-o", "--output", help="è¾“å‡ºåˆ°æ–‡ä»¶")
    stats_parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    # Markdownç­”å·æ‰¹æ”¹å‘½ä»¤
    markdown_parser = subparsers.add_parser('grade-markdown', help='æ‰¹æ”¹Markdownæ ¼å¼çš„ç­”å·')
    markdown_parser.add_argument("-f", "--file", required=True, help="Markdownç­”å·æ–‡ä»¶è·¯å¾„")
    markdown_parser.add_argument("-j", "--json", help="è¯•å·JSONæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«é¢˜ç›®IDæ˜ å°„ä¿¡æ¯")
    markdown_parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½• (é»˜è®¤: output)")
    markdown_parser.add_argument("--use-llm", action="store_true", help="ä½¿ç”¨LLMè¿›è¡Œå¢å¼ºè§£æ")
    markdown_parser.add_argument("--no-llm", action="store_true", help="ç¦ç”¨LLMè§£æï¼Œä»…ä½¿ç”¨è§„åˆ™è§£æ")
    markdown_parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    # åŸºäºæ˜“é”™ç‚¹ç”Ÿæˆç»ƒä¹ é¢˜ç›®å‘½ä»¤
    targeted_practice_parser = subparsers.add_parser('targeted-practice', help='åŸºäºæ˜“é”™ç‚¹ç”Ÿæˆé’ˆå¯¹æ€§ç»ƒä¹ é¢˜ç›®')
    targeted_practice_parser.add_argument("-f", "--file", required=True, help="Markdownç­”å·æ–‡ä»¶è·¯å¾„")
    targeted_practice_parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½• (é»˜è®¤: output)")
    targeted_practice_parser.add_argument("--choice-count", type=int, default=2, help="é€‰æ‹©é¢˜æ•°é‡ (é»˜è®¤: 2)")
    targeted_practice_parser.add_argument("--calculation-count", type=int, default=2, help="è®¡ç®—é¢˜æ•°é‡ (é»˜è®¤: 2)")
    targeted_practice_parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    # OCRè½¬Markdownå‘½ä»¤
    ocr_md_parser = subparsers.add_parser('ocr-to-markdown', help='ä½¿ç”¨LLM OCRå°†è¯•å·å›¾ç‰‡è¯†åˆ«ä¸ºMarkdownå†…å®¹')
    ocr_md_parser.add_argument("-i", "--image", required=True, help="è¾“å…¥å›¾ç‰‡è·¯å¾„")
    ocr_md_parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½• (é»˜è®¤: output)")
    ocr_md_parser.add_argument("--filename", default="ocr_result.md", help="è¾“å‡ºMarkdownæ–‡ä»¶å (é»˜è®¤: ocr_result.md)")
    ocr_md_parser.add_argument("--format", choices=['practice', 'exam', 'homework'], default='practice', 
                               help="è¯•å·æ ¼å¼ç±»å‹ (é»˜è®¤: practice)")
    ocr_md_parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æä¾›å‘½ä»¤ï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not args.command:
        parser.print_help()
        return
    
    if args.command == 'grade':
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.image):
            print(f"é”™è¯¯: å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {args.image}", file=sys.stderr)
            sys.exit(1)
    
    if args.command == 'grade-markdown':
        # æ£€æŸ¥Markdownæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.file):
            print(f"é”™è¯¯: Markdownæ–‡ä»¶ä¸å­˜åœ¨: {args.file}", file=sys.stderr)
            sys.exit(1)
    
    if args.command == 'targeted-practice':
        # æ£€æŸ¥Markdownæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.file):
            print(f"é”™è¯¯: Markdownæ–‡ä»¶ä¸å­˜åœ¨: {args.file}", file=sys.stderr)
            sys.exit(1)
    
    if args.command == 'ocr-to-markdown':
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.image):
            print(f"é”™è¯¯: å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {args.image}", file=sys.stderr)
            sys.exit(1)
    
    try:
        if args.command == 'grade':
            # Initialize grader
            grader = MathGrader(output_dir=args.output)
            
            # Process image
            results = grader.process_image(args.image)
            
            if "error" in results:
                print(f"å¤„ç†å¤±è´¥: {results['error']}", file=sys.stderr)
                if "details" in results:
                    print(f"è¯¦ç»†ä¿¡æ¯: {results['details']}", file=sys.stderr)
                sys.exit(1)
            
            # Save results
            grader.save_results(results)
            
            # Print summary
            summary = results['summary']
            print(f"\næ‰¹æ”¹å®Œæˆ!")
            print(f"æ€»é¢˜æ•°: {summary['total_problems']}")
            print(f"æ­£ç¡®é¢˜æ•°: {summary['correct_problems']}")
            print(f"å‡†ç¡®ç‡: {summary['accuracy_percentage']:.1f}%")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}/")
            
        elif args.command == 'practice':
            # Initialize grader for practice test generation
            grader = MathGrader(output_dir=args.output)
            
            # Generate practice test
            difficulty_range = None
            if args.difficulty_range:
                difficulty_range = tuple(args.difficulty_range)
            
            practice_test = grader.generate_practice_test(
                error_types=args.error_types,
                choice_count=args.choice_count,
                calculation_count=args.calculation_count,
                random_seed=args.random_seed
            )
            
            if "error" in practice_test:
                print(f"ç”Ÿæˆç»ƒä¹ è¯•å·å¤±è´¥: {practice_test['error']}", file=sys.stderr)
                sys.exit(1)
            
            # Save practice test
            grader.save_practice_test(practice_test)
            
            # Print summary
            print(f"\nç»ƒä¹ è¯•å·ç”Ÿæˆå®Œæˆ!")
            print(f"é”™è¯¯ç±»å‹: {', '.join(args.error_types)}")
            print(f"é€‰æ‹©é¢˜æ•°é‡: {args.choice_count}")
            print(f"è®¡ç®—é¢˜æ•°é‡: {args.calculation_count}")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}/")
            
        elif args.command == 'stats':
            # Initialize grader for statistics
            grader = MathGrader(output_dir=args.output if args.output else "output")
            
            # Generate statistics
            stats = grader.generate_question_bank_statistics()
            
            if "error" in stats:
                print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats['error']}", file=sys.stderr)
                sys.exit(1)
            
            # Display statistics
            if args.format == 'json':
                output = json.dumps(stats, ensure_ascii=False, indent=2)
            else:
                output = grader.format_statistics_table(stats)
            
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    f.write(output)
                print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {args.output}")
            else:
                print(output)
        
        elif args.command == 'grade-markdown':
            # Initialize markdown grader
            grader = MarkdownGrader(output_dir=args.output)
            
            # Read markdown file
            with open(args.file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()
            
            # Load question mapping from JSON file if provided
            question_mapping = None
            if args.json:
                if not os.path.exists(args.json):
                    print(f"é”™è¯¯: JSONæ–‡ä»¶ä¸å­˜åœ¨: {args.json}", file=sys.stderr)
                    sys.exit(1)
                
                try:
                    with open(args.json, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                    # ä¼˜å…ˆä½¿ç”¨questionsæ•°ç»„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨question_mapping
                    questions = json_data.get('questions', [])
                    question_mapping = json_data.get('question_mapping', {})
                    
                    if questions:
                        print(f"ä»JSONæ–‡ä»¶åŠ è½½questionsæ•°ç»„: {len(questions)} é“é¢˜ç›®")
                    elif question_mapping:
                        print(f"ä»JSONæ–‡ä»¶åŠ è½½é¢˜ç›®æ˜ å°„: {len(question_mapping)} é“é¢˜ç›®")
                except Exception as e:
                    print(f"é”™è¯¯: æ— æ³•è¯»å–JSONæ–‡ä»¶: {e}", file=sys.stderr)
                    sys.exit(1)
            
            # Debug: æ‰“å°å‚æ•°å€¼
            if args.verbose:
                print(f"è°ƒè¯•ä¿¡æ¯: use_llm={args.use_llm}, no_llm={args.no_llm}")
                if question_mapping:
                    print(f"é¢˜ç›®æ˜ å°„: {question_mapping}")
            
            # Check for conflicting arguments
            if args.use_llm and args.no_llm:
                print("é”™è¯¯: ä¸èƒ½åŒæ—¶æŒ‡å®š --use-llm å’Œ --no-llm é€‰é¡¹", file=sys.stderr)
                sys.exit(1)
            
            # Determine parsing method
            if args.no_llm:
                use_llm = False
                print("é€‰æ‹©è§„åˆ™è§£æï¼ˆ--no-llmï¼‰")
            elif args.use_llm:
                use_llm = True
                print("é€‰æ‹©LLMè§£æï¼ˆ--use-llmï¼‰")
            else:
                use_llm = True  # é»˜è®¤ä½¿ç”¨LLM
                print("é€‰æ‹©LLMè§£æï¼ˆé»˜è®¤ï¼‰")
            
            if args.verbose:
                if use_llm:
                    print("ä½¿ç”¨LLMè¿›è¡Œå¢å¼ºè§£æ...")
                else:
                    print("ä½¿ç”¨è§„åˆ™è§£æ...")
            
            # Parse and grade markdown test with question mapping
            markdown_test = grader.parse_markdown_test(markdown_content, use_llm=use_llm, question_mapping=question_mapping, questions=questions)
            results = grader.grade_markdown_test(markdown_test)
            
            # Save results
            grader.save_grading_results(results)
            
            # Print summary
            summary = results['grading_summary']
            print(f"\nMarkdownç­”å·æ‰¹æ”¹å®Œæˆ!")
            print(f"æ€»é¢˜æ•°: {summary['total_questions']}")
            print(f"æ­£ç¡®é¢˜æ•°: {summary['correct_questions']}")
            print(f"å‡†ç¡®ç‡: {summary['accuracy_percentage']:.1f}%")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}/")
            
            # Print error analysis
            error_analysis = results.get('error_analysis', {})
            if error_analysis.get('weak_areas'):
                print(f"\næ˜“é”™ç‚¹åˆ†æ:")
                print(f"è–„å¼±ç¯èŠ‚: {', '.join(error_analysis['weak_areas'])}")
                if error_analysis.get('most_common_error'):
                    print(f"æœ€å¸¸è§é”™è¯¯: {error_analysis['most_common_error']}")
        
        elif args.command == 'targeted-practice':
            # Initialize markdown grader
            grader = MarkdownGrader(output_dir=args.output)
            
            # Read markdown file
            with open(args.file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()
            
            # Parse and grade markdown test to get error analysis
            markdown_test = grader.parse_markdown_test(markdown_content)
            results = grader.grade_markdown_test(markdown_test)
            
            # Generate targeted practice based on error analysis
            error_analysis = results.get('error_analysis', {})
            practice_test = grader.generate_practice_from_errors(
                error_analysis=error_analysis,
                choice_count=args.choice_count,
                calculation_count=args.calculation_count
            )
            
            if "error" in practice_test:
                print(f"ç”Ÿæˆé’ˆå¯¹æ€§ç»ƒä¹ å¤±è´¥: {practice_test['error']}", file=sys.stderr)
                sys.exit(1)
            
            # Save practice test
            grader.save_practice_test(practice_test)
            
            # Print summary
            test_info = practice_test.get('test_info', {})
            print(f"\né’ˆå¯¹æ€§ç»ƒä¹ é¢˜ç›®ç”Ÿæˆå®Œæˆ!")
            print(f"åŸºäºæ˜“é”™ç‚¹: {', '.join(practice_test.get('targeted_weak_areas', []))}")
            print(f"é”™è¯¯ç±»å‹: {', '.join(practice_test.get('practice_error_types', []))}")
            print(f"é¢˜ç›®æ€»æ•°: {test_info.get('total_questions', 0)}")
            print(f"é€‰æ‹©é¢˜: {test_info.get('choice_questions', 0)} é“")
            print(f"è®¡ç®—é¢˜: {test_info.get('calculation_questions', 0)} é“")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}/")
        
        elif args.command == 'ocr-to-markdown':
            # Initialize grader for OCR to Markdown conversion
            grader = MathGrader(output_dir=args.output)
            
            # Convert image to Markdown
            result = grader.convert_image_to_markdown(
                image_path=args.image,
                format_type=args.format,
                output_filename=args.filename
            )
            
            if "error" in result:
                print(f"OCRè½¬Markdownå¤±è´¥: {result['error']}", file=sys.stderr)
                sys.exit(1)
            
            # Print summary
            print(f"\nOCRè½¬Markdownå®Œæˆ!")
            print(f"è¾“å…¥å›¾ç‰‡: {args.image}")
            print(f"è¾“å‡ºæ ¼å¼: {args.format}")
            print(f"è¾“å‡ºæ–‡ä»¶: {args.output}/{args.filename}")
            print(f"è¯†åˆ«ç½®ä¿¡åº¦: {result.get('confidence', 0):.1f}%")
            print(f"è¯†åˆ«æ–¹æ³•: {result.get('method', 'unknown')}")
        
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
